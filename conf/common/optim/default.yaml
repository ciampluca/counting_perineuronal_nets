# @package optim

resume: false

optimizer:
    _target_: torch.optim.Adam
    lr: 0.01

lr_scheduler:
    _target_: torch.optim.lr_scheduler.MultiStepLR
    gamma: 0.1

batch_size: 8
batch_accumulation: 1
epochs: 100

val_freq: 1
val_batch_size: ${.batch_size}
val_device: cuda

num_workers: 16

debug: true
debug_freq: 700
log_every: 1