# @package _global_

optim:
    resume: false

    loss:
        rank:
            margin: 0.1
            weight: 1
        spread:
            temperature: 5
            weight: 0
        classification:
            weight: 1
        kl_div:
            bins: 10
            eps: 1e-6

    optimizer:
        _target_: torch.optim.Adam
        lr: 0.001

    lr_scheduler:
        _target_: torch.optim.lr_scheduler.MultiStepLR
        milestones: [750]
        gamma: 0.1

    batch_size: 32
    batch_accumulation: 1
    epochs: 1000

    val_freq: 1
    val_batch_size: ${.batch_size}
    val_device: cpu

    num_workers: 8

    debug: true
    debug_freq: 700
    log_every: 1